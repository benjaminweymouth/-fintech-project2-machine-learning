{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### The Steps one to five below have been follwed in order to perform twitter analysis on the search terms for a perio dof 6 months i.e. April 2019 to September 2019. \n",
    "\n",
    "Step 1: Scrape tweets for a period of 6 months. This has resulted in approximately 43.7K tweets.\n",
    "\n",
    "Step 2: Added a list of relevant financial institutions to the tweets.\n",
    "\n",
    "Step 3: Extracted the relevant topics from each tweet using LDA for topic extraction.\n",
    "\n",
    "Step 4: Sentient extraction was performed using an LSTM based deep learning model.\n",
    "\n",
    "Step 5: Entity recognition was performed using thwe open source 3 class Stanford NER tagger i.e. Organization, Location and Names.\n",
    "\n",
    "#### The result of the analysis is visualised using tableau and is available via the below link:  https://public.tableau.com/profile/prajwal3523#!/vizhome/TwitterAnalysis_15704300313050/SummaryofTweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Scrape Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import GetOldTweets3 as got\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# NTLK functions\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize as tok\n",
    "from nltk.stem.snowball import SnowballStemmer # load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "import lda # topic modeling -NMF & LDA\n",
    "import string\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "# Load the LDA model from sk-learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "# Tf-Idf and Clustering packages\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_terms = ['mortgage','current account','savings account','insurance','credit card','pension',\n",
    "                'personal loan','money transfer','tax advice','investment','wealth management']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## scrape data from twitter for the above search terms\n",
    "tweet_df_all = pd.DataFrame()\n",
    "for term in search_terms:\n",
    "    print(term)\n",
    "    tweetCriteria = got.manager.TweetCriteria().setQuerySearch(term)\\\n",
    "                                               .setSince(\"2019-04-01\")\\\n",
    "                                               .setUntil(\"2019-09-30\")\\\n",
    "                                               .setNear(\"London\")\\\n",
    "                                               .setWithin(\"310mi\")\n",
    "    tweet = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "    tweet_list = [[tweet[x].id,\n",
    "                  tweet[x].author_id,\n",
    "                  tweet[x].text,\n",
    "                  tweet[x].retweets,\n",
    "                  tweet[x].permalink,\n",
    "                  tweet[x].date,\n",
    "                  tweet[x].formatted_date,\n",
    "                  tweet[x].favorites,\n",
    "                  tweet[x].mentions,\n",
    "                  tweet[x].hashtags,\n",
    "                  tweet[x].geo,\n",
    "                  tweet[x].urls\n",
    "                 ]for x in range(0, len(tweet))]\n",
    "    tweet_df = pd.DataFrame(tweet_list)\n",
    "    tweet_df['search_term'] = term\n",
    "    tweet_df_all = tweet_df_all.append(tweet_df)\n",
    "\n",
    "tweet_df_all.columns = ['id','author_id','text','retweets','permalink','date','formatted_date','favorites','mentions','hashtags','geo','urls']\n",
    "tweet_df_all.to_csv('../data/all_tweets.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Add List of Financial Institutions providng the above products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_df_all = pd.read_csv('../data/all_tweets.csv')\n",
    "tweet_df_all = tweet_df_all[tweet_df_all['text'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43704, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweets</th>\n",
       "      <th>permalink</th>\n",
       "      <th>date</th>\n",
       "      <th>formatted_date</th>\n",
       "      <th>favorites</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>geo</th>\n",
       "      <th>urls</th>\n",
       "      <th>search_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1178457108276289536</td>\n",
       "      <td>40080176</td>\n",
       "      <td>This normalisation of no deal is horrendous. P...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/KatarinaKeys/status/117845...</td>\n",
       "      <td>2019-09-29 23:50:43+00:00</td>\n",
       "      <td>Sun Sep 29 23:50:43 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1178455823242035201</td>\n",
       "      <td>1126071201481334787</td>\n",
       "      <td>Jumbo Mortgage Program https:// conclud.com/ht...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/Conclud2/status/1178455823...</td>\n",
       "      <td>2019-09-29 23:45:37+00:00</td>\n",
       "      <td>Sun Sep 29 23:45:37 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://conclud.com/https-www-madisonmortgageg...</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1178450126219685893</td>\n",
       "      <td>729387514914603009</td>\n",
       "      <td>If you have no work it's harder to feed your k...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/cjhenrygonzo/status/117845...</td>\n",
       "      <td>2019-09-29 23:22:59+00:00</td>\n",
       "      <td>Sun Sep 29 23:22:59 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1178446295985541120</td>\n",
       "      <td>1697126574</td>\n",
       "      <td>Solution. \"You'll need to be: 18+ and a UK res...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/blazedstorm/status/1178446...</td>\n",
       "      <td>2019-09-29 23:07:46+00:00</td>\n",
       "      <td>Sun Sep 29 23:07:46 +0000 2019</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1178446170722619393</td>\n",
       "      <td>1239955070</td>\n",
       "      <td>Kabaddi x3 UK Premier 1st show House Full Show...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/habamoment/status/11784461...</td>\n",
       "      <td>2019-09-29 23:07:16+00:00</td>\n",
       "      <td>Sun Sep 29 23:07:16 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>@Peepal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.facebook.com/habteam/posts/1106547...</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id            author_id  \\\n",
       "0  1178457108276289536             40080176   \n",
       "1  1178455823242035201  1126071201481334787   \n",
       "2  1178450126219685893   729387514914603009   \n",
       "3  1178446295985541120           1697126574   \n",
       "4  1178446170722619393           1239955070   \n",
       "\n",
       "                                                text  retweets  \\\n",
       "0  This normalisation of no deal is horrendous. P...         0   \n",
       "1  Jumbo Mortgage Program https:// conclud.com/ht...         0   \n",
       "2  If you have no work it's harder to feed your k...         0   \n",
       "3  Solution. \"You'll need to be: 18+ and a UK res...         0   \n",
       "4  Kabaddi x3 UK Premier 1st show House Full Show...         0   \n",
       "\n",
       "                                           permalink  \\\n",
       "0  https://twitter.com/KatarinaKeys/status/117845...   \n",
       "1  https://twitter.com/Conclud2/status/1178455823...   \n",
       "2  https://twitter.com/cjhenrygonzo/status/117845...   \n",
       "3  https://twitter.com/blazedstorm/status/1178446...   \n",
       "4  https://twitter.com/habamoment/status/11784461...   \n",
       "\n",
       "                        date                  formatted_date  favorites  \\\n",
       "0  2019-09-29 23:50:43+00:00  Sun Sep 29 23:50:43 +0000 2019          0   \n",
       "1  2019-09-29 23:45:37+00:00  Sun Sep 29 23:45:37 +0000 2019          0   \n",
       "2  2019-09-29 23:22:59+00:00  Sun Sep 29 23:22:59 +0000 2019          0   \n",
       "3  2019-09-29 23:07:46+00:00  Sun Sep 29 23:07:46 +0000 2019          2   \n",
       "4  2019-09-29 23:07:16+00:00  Sun Sep 29 23:07:16 +0000 2019          0   \n",
       "\n",
       "  mentions hashtags  geo                                               urls  \\\n",
       "0      NaN      NaN  NaN                                                NaN   \n",
       "1      NaN      NaN  NaN  https://conclud.com/https-www-madisonmortgageg...   \n",
       "2      NaN      NaN  NaN                                                NaN   \n",
       "3      NaN      NaN  NaN                                                NaN   \n",
       "4  @Peepal      NaN  NaN  https://www.facebook.com/habteam/posts/1106547...   \n",
       "\n",
       "  search_term  \n",
       "0    mortgage  \n",
       "1    mortgage  \n",
       "2    mortgage  \n",
       "3    mortgage  \n",
       "4    mortgage  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tweet_df_all.shape);tweet_df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fin_sector</th>\n",
       "      <th>fin_comp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Banks</td>\n",
       "      <td>Barclays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Banks</td>\n",
       "      <td>Lloyds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Banks</td>\n",
       "      <td>HSBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Banks</td>\n",
       "      <td>Citi Bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Banks</td>\n",
       "      <td>Santander</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  fin_sector   fin_comp\n",
       "0      Banks   Barclays\n",
       "1      Banks     Lloyds\n",
       "2      Banks       HSBC\n",
       "3      Banks  Citi Bank\n",
       "4      Banks  Santander"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the financail services comoanies list from csv\n",
    "fin_inst = pd.read_csv('../data/fin_serv.csv')\n",
    "tweet_df_all['text'] = tweet_df_all['text'].str.lower()\n",
    "tweet_df_all['company']=''\n",
    "fin_inst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "barclays\n",
      "lloyds\n",
      "hsbc\n",
      "citi bank\n",
      "santander\n",
      "nationwide\n",
      "bupa\n",
      "axa\n",
      "allianz\n",
      "zurich insurance group\n",
      "qbe\n",
      "direct line\n",
      "monzo\n",
      "starling\n",
      "revolut\n",
      "transfer wise\n",
      "western union\n"
     ]
    }
   ],
   "source": [
    "# Search through the data frame and find the companies\n",
    "for comp in fin_inst['fin_comp'].unique():\n",
    "    print(comp.lower())\n",
    "    tweet_df_all.loc[tweet_df_all['text'].str.contains(comp.lower()),'company']=comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>1178457108276289536</td>\n",
       "      <td>1178455823242035201</td>\n",
       "      <td>1178450126219685893</td>\n",
       "      <td>1178446295985541120</td>\n",
       "      <td>1178446170722619393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author_id</th>\n",
       "      <td>40080176</td>\n",
       "      <td>1126071201481334787</td>\n",
       "      <td>729387514914603009</td>\n",
       "      <td>1697126574</td>\n",
       "      <td>1239955070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>this normalisation of no deal is horrendous. p...</td>\n",
       "      <td>jumbo mortgage program https:// conclud.com/ht...</td>\n",
       "      <td>if you have no work it's harder to feed your k...</td>\n",
       "      <td>solution. \"you'll need to be: 18+ and a uk res...</td>\n",
       "      <td>kabaddi x3 uk premier 1st show house full show...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>retweets</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>permalink</th>\n",
       "      <td>https://twitter.com/KatarinaKeys/status/117845...</td>\n",
       "      <td>https://twitter.com/Conclud2/status/1178455823...</td>\n",
       "      <td>https://twitter.com/cjhenrygonzo/status/117845...</td>\n",
       "      <td>https://twitter.com/blazedstorm/status/1178446...</td>\n",
       "      <td>https://twitter.com/habamoment/status/11784461...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>2019-09-29 23:50:43+00:00</td>\n",
       "      <td>2019-09-29 23:45:37+00:00</td>\n",
       "      <td>2019-09-29 23:22:59+00:00</td>\n",
       "      <td>2019-09-29 23:07:46+00:00</td>\n",
       "      <td>2019-09-29 23:07:16+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>formatted_date</th>\n",
       "      <td>Sun Sep 29 23:50:43 +0000 2019</td>\n",
       "      <td>Sun Sep 29 23:45:37 +0000 2019</td>\n",
       "      <td>Sun Sep 29 23:22:59 +0000 2019</td>\n",
       "      <td>Sun Sep 29 23:07:46 +0000 2019</td>\n",
       "      <td>Sun Sep 29 23:07:16 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>favorites</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mentions</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@Peepal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hashtags</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geo</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>urls</th>\n",
       "      <td>NaN</td>\n",
       "      <td>https://conclud.com/https-www-madisonmortgageg...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.facebook.com/habteam/posts/1106547...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search_term</th>\n",
       "      <td>mortgage</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>company</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fin_sector</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fin_comp</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                0  \\\n",
       "id                                            1178457108276289536   \n",
       "author_id                                                40080176   \n",
       "text            this normalisation of no deal is horrendous. p...   \n",
       "retweets                                                        0   \n",
       "permalink       https://twitter.com/KatarinaKeys/status/117845...   \n",
       "date                                    2019-09-29 23:50:43+00:00   \n",
       "formatted_date                     Sun Sep 29 23:50:43 +0000 2019   \n",
       "favorites                                                       0   \n",
       "mentions                                                      NaN   \n",
       "hashtags                                                      NaN   \n",
       "geo                                                           NaN   \n",
       "urls                                                          NaN   \n",
       "search_term                                              mortgage   \n",
       "company                                                             \n",
       "fin_sector                                                    NaN   \n",
       "fin_comp                                                      NaN   \n",
       "\n",
       "                                                                1  \\\n",
       "id                                            1178455823242035201   \n",
       "author_id                                     1126071201481334787   \n",
       "text            jumbo mortgage program https:// conclud.com/ht...   \n",
       "retweets                                                        0   \n",
       "permalink       https://twitter.com/Conclud2/status/1178455823...   \n",
       "date                                    2019-09-29 23:45:37+00:00   \n",
       "formatted_date                     Sun Sep 29 23:45:37 +0000 2019   \n",
       "favorites                                                       0   \n",
       "mentions                                                      NaN   \n",
       "hashtags                                                      NaN   \n",
       "geo                                                           NaN   \n",
       "urls            https://conclud.com/https-www-madisonmortgageg...   \n",
       "search_term                                              mortgage   \n",
       "company                                                             \n",
       "fin_sector                                                    NaN   \n",
       "fin_comp                                                      NaN   \n",
       "\n",
       "                                                                2  \\\n",
       "id                                            1178450126219685893   \n",
       "author_id                                      729387514914603009   \n",
       "text            if you have no work it's harder to feed your k...   \n",
       "retweets                                                        0   \n",
       "permalink       https://twitter.com/cjhenrygonzo/status/117845...   \n",
       "date                                    2019-09-29 23:22:59+00:00   \n",
       "formatted_date                     Sun Sep 29 23:22:59 +0000 2019   \n",
       "favorites                                                       0   \n",
       "mentions                                                      NaN   \n",
       "hashtags                                                      NaN   \n",
       "geo                                                           NaN   \n",
       "urls                                                          NaN   \n",
       "search_term                                              mortgage   \n",
       "company                                                             \n",
       "fin_sector                                                    NaN   \n",
       "fin_comp                                                      NaN   \n",
       "\n",
       "                                                                3  \\\n",
       "id                                            1178446295985541120   \n",
       "author_id                                              1697126574   \n",
       "text            solution. \"you'll need to be: 18+ and a uk res...   \n",
       "retweets                                                        0   \n",
       "permalink       https://twitter.com/blazedstorm/status/1178446...   \n",
       "date                                    2019-09-29 23:07:46+00:00   \n",
       "formatted_date                     Sun Sep 29 23:07:46 +0000 2019   \n",
       "favorites                                                       2   \n",
       "mentions                                                      NaN   \n",
       "hashtags                                                      NaN   \n",
       "geo                                                           NaN   \n",
       "urls                                                          NaN   \n",
       "search_term                                              mortgage   \n",
       "company                                                             \n",
       "fin_sector                                                    NaN   \n",
       "fin_comp                                                      NaN   \n",
       "\n",
       "                                                                4  \n",
       "id                                            1178446170722619393  \n",
       "author_id                                              1239955070  \n",
       "text            kabaddi x3 uk premier 1st show house full show...  \n",
       "retweets                                                        0  \n",
       "permalink       https://twitter.com/habamoment/status/11784461...  \n",
       "date                                    2019-09-29 23:07:16+00:00  \n",
       "formatted_date                     Sun Sep 29 23:07:16 +0000 2019  \n",
       "favorites                                                       0  \n",
       "mentions                                                  @Peepal  \n",
       "hashtags                                                      NaN  \n",
       "geo                                                           NaN  \n",
       "urls            https://www.facebook.com/habteam/posts/1106547...  \n",
       "search_term                                              mortgage  \n",
       "company                                                            \n",
       "fin_sector                                                    NaN  \n",
       "fin_comp                                                      NaN  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df_comp = tweet_df_all.merge(fin_inst, how='left', left_on='company', right_on='fin_comp')\n",
    "tweet_df_comp.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "search_term\n",
       "credit card           2226\n",
       "current account        254\n",
       "insurance            10814\n",
       "investment           15673\n",
       "money transfer          69\n",
       "mortgage              4704\n",
       "pension               9347\n",
       "peronal loan            20\n",
       "savings account        182\n",
       "tax advice             146\n",
       "wealth management      269\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df_comp.groupby('search_term')['id'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Topic Extraction with LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove unnessary words\n",
    "#Complie all regular expressions\n",
    "isURL = re.compile(r'http[s]?:// (?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', re.VERBOSE | re.IGNORECASE)\n",
    "isRTusername = re.compile(r'^RT+[\\s]+(@[\\w_]+:)',re.VERBOSE | re.IGNORECASE) #r'^RT+[\\s]+(@[\\w_]+:)'\n",
    "isEntity = re.compile(r'@[\\w_]+', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "# Helper functions\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]])) \n",
    "\n",
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer, lda_model, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "     \n",
    "        \n",
    "def clean_tweet(row):\n",
    "    row = isURL.sub(\"\",row)\n",
    "    row = isRTusername.sub(\"\",row)\n",
    "    row = isEntity.sub(\"\",row)\n",
    "    return row\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in tok.sent_tokenize(text) for word in tok.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweets</th>\n",
       "      <th>permalink</th>\n",
       "      <th>date</th>\n",
       "      <th>formatted_date</th>\n",
       "      <th>favorites</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>geo</th>\n",
       "      <th>urls</th>\n",
       "      <th>search_term</th>\n",
       "      <th>company</th>\n",
       "      <th>fin_sector</th>\n",
       "      <th>fin_comp</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1178457108276289536</td>\n",
       "      <td>40080176</td>\n",
       "      <td>this normalisation of no deal is horrendous. p...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/KatarinaKeys/status/117845...</td>\n",
       "      <td>2019-09-29 23:50:43+00:00</td>\n",
       "      <td>Sun Sep 29 23:50:43 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mortgage</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this normalisation of no deal is horrendous pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1178455823242035201</td>\n",
       "      <td>1126071201481334787</td>\n",
       "      <td>jumbo mortgage program https:// conclud.com/ht...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/Conclud2/status/1178455823...</td>\n",
       "      <td>2019-09-29 23:45:37+00:00</td>\n",
       "      <td>Sun Sep 29 23:45:37 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://conclud.com/https-www-madisonmortgageg...</td>\n",
       "      <td>mortgage</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jumbo mortgage program https concludcomhttpsww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1178450126219685893</td>\n",
       "      <td>729387514914603009</td>\n",
       "      <td>if you have no work it's harder to feed your k...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/cjhenrygonzo/status/117845...</td>\n",
       "      <td>2019-09-29 23:22:59+00:00</td>\n",
       "      <td>Sun Sep 29 23:22:59 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mortgage</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>if you have no work its harder to feed your ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1178446295985541120</td>\n",
       "      <td>1697126574</td>\n",
       "      <td>solution. \"you'll need to be: 18+ and a uk res...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/blazedstorm/status/1178446...</td>\n",
       "      <td>2019-09-29 23:07:46+00:00</td>\n",
       "      <td>Sun Sep 29 23:07:46 +0000 2019</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mortgage</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>solution youll need to be 18 and a uk resident...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1178446170722619393</td>\n",
       "      <td>1239955070</td>\n",
       "      <td>kabaddi x3 uk premier 1st show house full show...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/habamoment/status/11784461...</td>\n",
       "      <td>2019-09-29 23:07:16+00:00</td>\n",
       "      <td>Sun Sep 29 23:07:16 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>@Peepal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.facebook.com/habteam/posts/1106547...</td>\n",
       "      <td>mortgage</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kabaddi x3 uk premier 1st show house full show...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id            author_id  \\\n",
       "0  1178457108276289536             40080176   \n",
       "1  1178455823242035201  1126071201481334787   \n",
       "2  1178450126219685893   729387514914603009   \n",
       "3  1178446295985541120           1697126574   \n",
       "4  1178446170722619393           1239955070   \n",
       "\n",
       "                                                text  retweets  \\\n",
       "0  this normalisation of no deal is horrendous. p...         0   \n",
       "1  jumbo mortgage program https:// conclud.com/ht...         0   \n",
       "2  if you have no work it's harder to feed your k...         0   \n",
       "3  solution. \"you'll need to be: 18+ and a uk res...         0   \n",
       "4  kabaddi x3 uk premier 1st show house full show...         0   \n",
       "\n",
       "                                           permalink  \\\n",
       "0  https://twitter.com/KatarinaKeys/status/117845...   \n",
       "1  https://twitter.com/Conclud2/status/1178455823...   \n",
       "2  https://twitter.com/cjhenrygonzo/status/117845...   \n",
       "3  https://twitter.com/blazedstorm/status/1178446...   \n",
       "4  https://twitter.com/habamoment/status/11784461...   \n",
       "\n",
       "                        date                  formatted_date  favorites  \\\n",
       "0  2019-09-29 23:50:43+00:00  Sun Sep 29 23:50:43 +0000 2019          0   \n",
       "1  2019-09-29 23:45:37+00:00  Sun Sep 29 23:45:37 +0000 2019          0   \n",
       "2  2019-09-29 23:22:59+00:00  Sun Sep 29 23:22:59 +0000 2019          0   \n",
       "3  2019-09-29 23:07:46+00:00  Sun Sep 29 23:07:46 +0000 2019          2   \n",
       "4  2019-09-29 23:07:16+00:00  Sun Sep 29 23:07:16 +0000 2019          0   \n",
       "\n",
       "  mentions hashtags  geo                                               urls  \\\n",
       "0      NaN      NaN  NaN                                                NaN   \n",
       "1      NaN      NaN  NaN  https://conclud.com/https-www-madisonmortgageg...   \n",
       "2      NaN      NaN  NaN                                                NaN   \n",
       "3      NaN      NaN  NaN                                                NaN   \n",
       "4  @Peepal      NaN  NaN  https://www.facebook.com/habteam/posts/1106547...   \n",
       "\n",
       "  search_term company fin_sector fin_comp  \\\n",
       "0    mortgage                NaN      NaN   \n",
       "1    mortgage                NaN      NaN   \n",
       "2    mortgage                NaN      NaN   \n",
       "3    mortgage                NaN      NaN   \n",
       "4    mortgage                NaN      NaN   \n",
       "\n",
       "                                          text_clean  \n",
       "0  this normalisation of no deal is horrendous pe...  \n",
       "1  jumbo mortgage program https concludcomhttpsww...  \n",
       "2  if you have no work its harder to feed your ki...  \n",
       "3  solution youll need to be 18 and a uk resident...  \n",
       "4  kabaddi x3 uk premier 1st show house full show...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove urls and retweets and entities from the text\n",
    "tweet_df_comp['text_clean'] = tweet_df_comp['text'].apply(lambda row:clean_tweet(row))\n",
    "\n",
    "#remove punctuations\n",
    "RE_PUNCTUATION = '|'.join([re.escape(x) for x in string.punctuation])  \n",
    "tweet_df_comp['text_clean'] = tweet_df_comp['text_clean'].str.replace(RE_PUNCTUATION, \"\")\n",
    "tweet_df_comp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of stopwords\n",
    "stop_words= stopwords.words('english') #import stopwords from NLTK package\n",
    "readInStopwords = pd.read_csv(\"../data/pre_process/twitterStopWords.csv\", encoding='ISO-8859-1') # import stopwords from CSV file as pandas data frame\n",
    "readInStopwords = readInStopwords.wordList.tolist() # convert pandas data frame to a list\n",
    "readInStopwords.append('http')\n",
    "readInStopwords.append('https')\n",
    "\n",
    "# add in search terms as topic extraction is performed within each search topic, \n",
    "# we do not want the word or valriation of the word captured as a topic word\n",
    "search_terms_revised = ['mortgages','wealthmanagement','pensions','money','transfer']\n",
    "readInStopwords.extend(search_terms)\n",
    "readInStopwords.extend(search_terms_revised)\n",
    "\n",
    "stop_list = stop_words + readInStopwords # combine two lists i.e. NLTK stop words and CSV stopwords\n",
    "stop_list = list(set(stop_list)) # strore only unique values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameter for lda, i am selecrign 3 topic and 4 words for each of the search terms \n",
    "number_topics = 5\n",
    "number_words = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mortgage\n",
      "Topics found via LDA:\n",
      "     index   Word 0    Word 1     Word 2  Word 3    Word 4  topic_index\n",
      "0  Topic 0  adviser      work       jobs     pay      pass            0\n",
      "1  Topic 1     week        us      right  broker  business            1\n",
      "2  Topic 2      get    people        pay   years      home            2\n",
      "3  Topic 3    rates  interest  financial    bank      rate            3\n",
      "4  Topic 4     like       day      going     got       say            4\n",
      "current account\n",
      "Topics found via LDA:\n",
      "     index   Word 0   Word 1 Word 2    Word 3   Word 4  topic_index\n",
      "0  Topic 0  deficit     also   it’s      keep  banking            0\n",
      "1  Topic 1      one  savings   help  interest     card            1\n",
      "2  Topic 2     card       uk   it’s     today    using            2\n",
      "3  Topic 3  deficit      get    use     great     paid            3\n",
      "4  Topic 4       hi     bank    one      card   trying            4\n",
      "savings account\n",
      "Topics found via LDA:\n",
      "     index  Word 0    Word 1 Word 2    Word 3   Word 4  topic_index\n",
      "0  Topic 0     get       pay   work     don’t       go            0\n",
      "1  Topic 1  credit        hi   card  accounts   update            1\n",
      "2  Topic 2  paying     month   want      like      pay            2\n",
      "3  Topic 3    bank  interest    use      like  current            3\n",
      "4  Topic 4     got       put    ive     first     bank            4\n",
      "insurance\n",
      "Topics found via LDA:\n",
      "     index  Word 0 Word 1     Word 2    Word 3      Word 4  topic_index\n",
      "0  Topic 0  london   jobs    vehicle  business      driver            0\n",
      "1  Topic 1  travel   take        one      road  healthcare            1\n",
      "2  Topic 2    much   many  insurtech   working       costs            2\n",
      "3  Topic 3     get    car     health       pay      people            3\n",
      "4  Topic 4    year    tax        day   service      public            4\n",
      "credit card\n",
      "Topics found via LDA:\n",
      "     index    Word 0     Word 1  Word 2  Word 3 Word 4  topic_index\n",
      "0  Topic 0   details  customers  thomas    cook  clear            0\n",
      "1  Topic 1  customer    booking    cash      uk   ever            1\n",
      "2  Topic 2       pay       need   debit    back   debt            2\n",
      "3  Topic 3       get       it’s     one     see    new            3\n",
      "4  Topic 4       i’m        get    free  people   bank            4\n",
      "pension\n",
      "Topics found via LDA:\n",
      "     index  Word 0   Word 1 Word 2  Word 3      Word 4  topic_index\n",
      "0  Topic 0  please    house   jobs     mps        last            0\n",
      "1  Topic 1      eu    state    pay  people         get            1\n",
      "2  Topic 2     tax     year   paid  income      salary            2\n",
      "3  Topic 3   staff  looking   find      de  racecourse            3\n",
      "4  Topic 4   funds     fund   need  credit     whether            4\n",
      "peronal loan\n",
      "Topics found via LDA:\n",
      "     index        Word 0    Word 1         Word 2           Word 3  \\\n",
      "0  Topic 0        driver  customer           taxi         becoming   \n",
      "1  Topic 1         spend   foreign  discretionary             card   \n",
      "2  Topic 2  selfemployed      know           read  personalfinance   \n",
      "3  Topic 3      business     loans        purpose          fintech   \n",
      "4  Topic 4          bank      club       experian           normal   \n",
      "\n",
      "                    Word 4  topic_index  \n",
      "0                    maybe            0  \n",
      "1  pictwittercomx5icuwzlr5            1  \n",
      "2                     blog            2  \n",
      "3                     back            3  \n",
      "4                     paid            4  \n",
      "money transfer\n",
      "Topics found via LDA:\n",
      "     index         Word 0        Word 1         Word 2         Word 3 Word 4  \\\n",
      "0  Topic 0  moneytransfer          dexa        fintech            ieo     rt   \n",
      "1  Topic 1  moneytransfer            us   transferwise  international  banks   \n",
      "2  Topic 2       services            ho         crypto          years  don’t   \n",
      "3  Topic 3  moneytransfer         today            new            get    app   \n",
      "4  Topic 4  moneytransfer  transferwise  international    remittances  check   \n",
      "\n",
      "   topic_index  \n",
      "0            0  \n",
      "1            1  \n",
      "2            2  \n",
      "3            3  \n",
      "4            4  \n",
      "tax advice\n",
      "Topics found via LDA:\n",
      "     index     Word 0     Word 1     Word 2      Word 3     Word 4  \\\n",
      "0  Topic 0    council   benefits   interest         ppi     paying   \n",
      "1  Topic 1       hmrc       call    probate      people  avoidance   \n",
      "2  Topic 2   business       need      great  birmingham      thank   \n",
      "3  Topic 3        get  avoidance  taxadvice        take      issue   \n",
      "4  Topic 4  taxadvice       week     payers        high       read   \n",
      "\n",
      "   topic_index  \n",
      "0            0  \n",
      "1            1  \n",
      "2            2  \n",
      "3            3  \n",
      "4            4  \n",
      "investment\n",
      "Topics found via LDA:\n",
      "     index       Word 0       Word 1       Word 2    Word 3    Word 4  \\\n",
      "0  Topic 0      killing         stop        spies  gererals   princes   \n",
      "1  Topic 1           uk         time  investments    people       get   \n",
      "2  Topic 2  investments         read        value    season     green   \n",
      "3  Topic 3          new       public         even     great    social   \n",
      "4  Topic 4     property  opportunity  investments       buy  investor   \n",
      "\n",
      "   topic_index  \n",
      "0            0  \n",
      "1            1  \n",
      "2            2  \n",
      "3            3  \n",
      "4            4  \n",
      "wealth management\n",
      "Topics found via LDA:\n",
      "     index  Word 0               Word 1            Word 2       Word 3  \\\n",
      "0  Topic 0  wealth           management           finance         logo   \n",
      "1  Topic 1  robo19  businessdevelopment  performancecoach  investments   \n",
      "2  Topic 2    list                nviso        wealthtech      fintech   \n",
      "3  Topic 3  wealth           management          business           uk   \n",
      "4  Topic 4  wealth           management                uk       strong   \n",
      "\n",
      "       Word 4  topic_index  \n",
      "0          us            0  \n",
      "1         new            1  \n",
      "2         one            2  \n",
      "3       great            3  \n",
      "4  highlights            4  \n"
     ]
    }
   ],
   "source": [
    "tweets_all_topics= pd.DataFrame()\n",
    "# term frequency modelling\n",
    "for terms in tweet_df_comp['search_term'].unique():\n",
    "    print(terms)\n",
    "    tweets_search_topics  = tweet_df_comp[tweet_df_comp['search_term']==terms].reset_index(drop=True)\n",
    "    corpus = tweets_search_topics['text_clean'].tolist()\n",
    "    # print(corpus)\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.9, min_df=0.00, stop_words=stop_list, tokenizer=tokenize_only) # Use tf (raw term count) features for LDA.\n",
    "    tf = tf_vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    # Create and fit the LDA model\n",
    "    model = LDA(n_components=number_topics, n_jobs=-1)\n",
    "    id_topic = model.fit(tf)\n",
    "    # Print the topics found by the LDA model\n",
    "    print(\"Topics found via LDA:\")\n",
    "    topic_keywords = show_topics(vectorizer=tf_vectorizer, lda_model=model, n_words=number_words)        \n",
    "    # Topic - Keywords Dataframe\n",
    "    df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "    df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "    df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "    df_topic_keywords = df_topic_keywords.reset_index()\n",
    "    df_topic_keywords['topic_index'] = df_topic_keywords['index'].str.split(' ', n = 1, expand = True)[[1]].astype('int')\n",
    "    print(df_topic_keywords)\n",
    "    \n",
    "    ############ get the dominat topic for each document in a data frame ###############\n",
    "    # Create Document — Topic Matrix\n",
    "    lda_output = model.transform(tf)\n",
    "    # column names\n",
    "    topicnames = [\"Topic\" + str(i) for i in range(model.n_components)]\n",
    "    # index names\n",
    "    docnames = [\"Doc\" + str(i) for i in range(len(corpus))]\n",
    "    \n",
    "    # Make the pandas dataframe\n",
    "    df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "    # Get dominant topic for each document\n",
    "    dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "    df_document_topic['dominant_topic'] = dominant_topic   \n",
    "    df_document_topic = df_document_topic.reset_index()\n",
    "        \n",
    "    #combine all the search terms into one data frame\n",
    "    tweets_topics = tweets_search_topics.merge(df_document_topic, left_index=True, right_index=True, how='left')\n",
    "    tweets_topics_words = tweets_topics.merge(df_topic_keywords, how='left', left_on='dominant_topic', right_on='topic_index')\n",
    "    tweets_all_topics = tweets_all_topics.append(tweets_topics_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43704, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweets</th>\n",
       "      <th>permalink</th>\n",
       "      <th>date</th>\n",
       "      <th>formatted_date</th>\n",
       "      <th>favorites</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>...</th>\n",
       "      <th>Topic3</th>\n",
       "      <th>Topic4</th>\n",
       "      <th>dominant_topic</th>\n",
       "      <th>index_y</th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>topic_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1178457108276289536</td>\n",
       "      <td>40080176</td>\n",
       "      <td>this normalisation of no deal is horrendous. p...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/KatarinaKeys/status/117845...</td>\n",
       "      <td>2019-09-29 23:50:43+00:00</td>\n",
       "      <td>Sun Sep 29 23:50:43 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.96</td>\n",
       "      <td>4</td>\n",
       "      <td>Topic 4</td>\n",
       "      <td>like</td>\n",
       "      <td>day</td>\n",
       "      <td>going</td>\n",
       "      <td>got</td>\n",
       "      <td>say</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1178455823242035201</td>\n",
       "      <td>1126071201481334787</td>\n",
       "      <td>jumbo mortgage program https:// conclud.com/ht...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/Conclud2/status/1178455823...</td>\n",
       "      <td>2019-09-29 23:45:37+00:00</td>\n",
       "      <td>Sun Sep 29 23:45:37 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.04</td>\n",
       "      <td>3</td>\n",
       "      <td>Topic 3</td>\n",
       "      <td>rates</td>\n",
       "      <td>interest</td>\n",
       "      <td>financial</td>\n",
       "      <td>bank</td>\n",
       "      <td>rate</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1178450126219685893</td>\n",
       "      <td>729387514914603009</td>\n",
       "      <td>if you have no work it's harder to feed your k...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/cjhenrygonzo/status/117845...</td>\n",
       "      <td>2019-09-29 23:22:59+00:00</td>\n",
       "      <td>Sun Sep 29 23:22:59 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>Topic 0</td>\n",
       "      <td>adviser</td>\n",
       "      <td>work</td>\n",
       "      <td>jobs</td>\n",
       "      <td>pay</td>\n",
       "      <td>pass</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1178446295985541120</td>\n",
       "      <td>1697126574</td>\n",
       "      <td>solution. \"you'll need to be: 18+ and a uk res...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/blazedstorm/status/1178446...</td>\n",
       "      <td>2019-09-29 23:07:46+00:00</td>\n",
       "      <td>Sun Sep 29 23:07:46 +0000 2019</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>Topic 3</td>\n",
       "      <td>rates</td>\n",
       "      <td>interest</td>\n",
       "      <td>financial</td>\n",
       "      <td>bank</td>\n",
       "      <td>rate</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1178446170722619393</td>\n",
       "      <td>1239955070</td>\n",
       "      <td>kabaddi x3 uk premier 1st show house full show...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/habamoment/status/11784461...</td>\n",
       "      <td>2019-09-29 23:07:16+00:00</td>\n",
       "      <td>Sun Sep 29 23:07:16 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>@Peepal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0</td>\n",
       "      <td>Topic 0</td>\n",
       "      <td>adviser</td>\n",
       "      <td>work</td>\n",
       "      <td>jobs</td>\n",
       "      <td>pay</td>\n",
       "      <td>pass</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id            author_id  \\\n",
       "0  1178457108276289536             40080176   \n",
       "1  1178455823242035201  1126071201481334787   \n",
       "2  1178450126219685893   729387514914603009   \n",
       "3  1178446295985541120           1697126574   \n",
       "4  1178446170722619393           1239955070   \n",
       "\n",
       "                                                text  retweets  \\\n",
       "0  this normalisation of no deal is horrendous. p...         0   \n",
       "1  jumbo mortgage program https:// conclud.com/ht...         0   \n",
       "2  if you have no work it's harder to feed your k...         0   \n",
       "3  solution. \"you'll need to be: 18+ and a uk res...         0   \n",
       "4  kabaddi x3 uk premier 1st show house full show...         0   \n",
       "\n",
       "                                           permalink  \\\n",
       "0  https://twitter.com/KatarinaKeys/status/117845...   \n",
       "1  https://twitter.com/Conclud2/status/1178455823...   \n",
       "2  https://twitter.com/cjhenrygonzo/status/117845...   \n",
       "3  https://twitter.com/blazedstorm/status/1178446...   \n",
       "4  https://twitter.com/habamoment/status/11784461...   \n",
       "\n",
       "                        date                  formatted_date  favorites  \\\n",
       "0  2019-09-29 23:50:43+00:00  Sun Sep 29 23:50:43 +0000 2019          0   \n",
       "1  2019-09-29 23:45:37+00:00  Sun Sep 29 23:45:37 +0000 2019          0   \n",
       "2  2019-09-29 23:22:59+00:00  Sun Sep 29 23:22:59 +0000 2019          0   \n",
       "3  2019-09-29 23:07:46+00:00  Sun Sep 29 23:07:46 +0000 2019          2   \n",
       "4  2019-09-29 23:07:16+00:00  Sun Sep 29 23:07:16 +0000 2019          0   \n",
       "\n",
       "  mentions hashtags  ...  Topic3 Topic4 dominant_topic  index_y   Word 0  \\\n",
       "0      NaN      NaN  ...    0.01   0.96              4  Topic 4     like   \n",
       "1      NaN      NaN  ...    0.84   0.04              3  Topic 3    rates   \n",
       "2      NaN      NaN  ...    0.01   0.01              0  Topic 0  adviser   \n",
       "3      NaN      NaN  ...    0.63   0.01              3  Topic 3    rates   \n",
       "4  @Peepal      NaN  ...    0.02   0.02              0  Topic 0  adviser   \n",
       "\n",
       "     Word 1     Word 2 Word 3  Word 4  topic_index  \n",
       "0       day      going    got     say            4  \n",
       "1  interest  financial   bank    rate            3  \n",
       "2      work       jobs    pay    pass            0  \n",
       "3  interest  financial   bank    rate            3  \n",
       "4      work       jobs    pay    pass            0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_all_topics = tweets_all_topics.reset_index(drop=True)\n",
    "print(tweets_all_topics.shape)\n",
    "tweets_all_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_all_topics.to_csv('../processed_data/tweets_all_topics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Sentiment analysis using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have trained the model usign the movie review data. The details of the training of the model can be found here: https://towardsdatascience.com/sentiment-analysis-for-text-with-deep-learning-2f0a0c6472b5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import h5py\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 56, 300)           120000300 \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               439296    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 120,576,310\n",
      "Trainable params: 576,010\n",
      "Non-trainable params: 120,000,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# read in the weight of the trained model.\n",
    "weight_path = '../models/dl_sentiment_model/best_weight_glove_bi_512.hdf5'\n",
    "prd_model = load_model(weight_path)\n",
    "prd_model.summary()\n",
    "word_idx = json.load(open(\"../models/dl_sentiment_model/word_idx.txt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentiment_DL(prd_model, text_data, word_idx):\n",
    "\n",
    "    #data = \"Pass the salt\"\n",
    "\n",
    "    live_list = []\n",
    "    batchSize = len(text_data)\n",
    "    live_list_np = np.zeros((56,batchSize))\n",
    "    for index, row in text_data.iterrows():\n",
    "        #print (index)\n",
    "        text_data_sample = text_data['text'][index]\n",
    "        # split the sentence into its words and remove any punctuations.\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        text_data_list = tokenizer.tokenize(text_data_sample)\n",
    "\n",
    "        #text_data_list = text_data_sample.split()\n",
    "\n",
    "\n",
    "        labels = np.array(['1','2','3','4','5','6','7','8','9','10'], dtype = \"int\")\n",
    "        #word_idx['I']\n",
    "        # get index for the live stage\n",
    "        data_index = np.array([word_idx[word.lower()] if word.lower() in word_idx else 0 for word in text_data_list])\n",
    "        data_index_np = np.array(data_index)\n",
    "\n",
    "        # padded with zeros of length 56 i.e maximum length\n",
    "        padded_array = np.zeros(56)\n",
    "        padded_array[:data_index_np.shape[0]] = data_index_np[:56]\n",
    "        data_index_np_pad = padded_array.astype(int)\n",
    "\n",
    "\n",
    "        live_list.append(data_index_np_pad)\n",
    "\n",
    "    live_list_np = np.asarray(live_list)\n",
    "    score = prd_model.predict(live_list_np, batch_size=batchSize, verbose=0)\n",
    "    single_score = np.round(np.dot(score, labels)/10,decimals=2)\n",
    "\n",
    "    score_all  = []\n",
    "    for each_score in score:\n",
    "\n",
    "        top_3_index = np.argsort(each_score)[-3:]\n",
    "        top_3_scores = each_score[top_3_index]\n",
    "        top_3_weights = top_3_scores/np.sum(top_3_scores)\n",
    "        single_score_dot = np.round(np.dot(top_3_index, top_3_weights)/10, decimals = 2)\n",
    "        score_all.append(single_score_dot)\n",
    "\n",
    "    text_data['Sentiment_Score'] = pd.DataFrame(score_all)\n",
    "\n",
    "    return text_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_data =  tweets_all_topics\n",
    "# Deep Learning sentiment scoring\n",
    "text_out = get_sentiment_DL(prd_model, text_data, word_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example of negative tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>37967</th>\n",
       "      <th>42412</th>\n",
       "      <th>30487</th>\n",
       "      <th>19344</th>\n",
       "      <th>14520</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>johnson was worse at delivery than khans and h...</td>\n",
       "      <td>i'm just so disappointed, i made a very, very ...</td>\n",
       "      <td>snp ministers lose public £135m in bad loans a...</td>\n",
       "      <td>48:00 \"additional pension is extremely poor va...</td>\n",
       "      <td>the very worst insurance company i have ever u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment_Score</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             37967  \\\n",
       "text             johnson was worse at delivery than khans and h...   \n",
       "Sentiment_Score                                               0.04   \n",
       "\n",
       "                                                             42412  \\\n",
       "text             i'm just so disappointed, i made a very, very ...   \n",
       "Sentiment_Score                                               0.04   \n",
       "\n",
       "                                                             30487  \\\n",
       "text             snp ministers lose public £135m in bad loans a...   \n",
       "Sentiment_Score                                               0.04   \n",
       "\n",
       "                                                             19344  \\\n",
       "text             48:00 \"additional pension is extremely poor va...   \n",
       "Sentiment_Score                                               0.04   \n",
       "\n",
       "                                                             14520  \n",
       "text             the very worst insurance company i have ever u...  \n",
       "Sentiment_Score                                               0.04  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_out.sort_values(by='Sentiment_Score')[['text','Sentiment_Score']].head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example of positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>34887</th>\n",
       "      <th>31525</th>\n",
       "      <th>20778</th>\n",
       "      <th>33675</th>\n",
       "      <th>6975</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>#patrizia tracks surge in first-half performan...</td>\n",
       "      <td>we are delighted to have won property manager ...</td>\n",
       "      <td>newham pensions fund awarded a ‘highly commend...</td>\n",
       "      <td>\"one of the finest clos apaltas of all time,\" ...</td>\n",
       "      <td>heartiest congratulations to team applied syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment_Score</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             34887  \\\n",
       "text             #patrizia tracks surge in first-half performan...   \n",
       "Sentiment_Score                                               0.87   \n",
       "\n",
       "                                                             31525  \\\n",
       "text             we are delighted to have won property manager ...   \n",
       "Sentiment_Score                                               0.87   \n",
       "\n",
       "                                                             20778  \\\n",
       "text             newham pensions fund awarded a ‘highly commend...   \n",
       "Sentiment_Score                                               0.86   \n",
       "\n",
       "                                                             33675  \\\n",
       "text             \"one of the finest clos apaltas of all time,\" ...   \n",
       "Sentiment_Score                                               0.86   \n",
       "\n",
       "                                                             6975   \n",
       "text             heartiest congratulations to team applied syst...  \n",
       "Sentiment_Score                                               0.85  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_out.sort_values(by='Sentiment_Score', ascending=False)[['text','Sentiment_Score']].head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the output files\n",
    "text_out.to_csv('../processed_data/tweets_topics_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5:  Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below section is implementing a stanford 3 class NER tagger. The model is trained based on on supervised Conditional Random Field (CRF) model. Additional information on the model is available at https://nlp.stanford.edu/software/CRF-NER.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_NER(text_data):\n",
    "    #/Users/prajwalshreyas/Desktop/Singularity/dockerApps/ner-algo/stanford-ner-2015-01-30\n",
    "    stanford_classifier = '../models/ner/english.all.3class.distsim.crf.ser.gz'\n",
    "    stanford_ner_path = '../models/ner/stanford-ner.jar'\n",
    "\n",
    "    #try:\n",
    "        # Creating Tagger Object\n",
    "    st = StanfordNERTagger(stanford_classifier, stanford_ner_path, encoding='utf-8')\n",
    "    #except Exception as e:\n",
    "    #       print (e)\n",
    "\n",
    "    # Get keyword for the input data frame\n",
    "    #keyword = tweetDataFrame.keyword.unique()\n",
    "    # Subset column containing tweet text and convert to list\n",
    "    # next insert a placeholder ' 12345678 ' to signify end of individual tweets\n",
    "\n",
    "    #text_data = pd.read_json('/Users/prajwalshreyas/Desktop/Singularity/dockerApps/sentiment-algo/app-sentiment-algo/sample_text.json')\n",
    "    print ('start get_NER')\n",
    "    text_out = text_data.copy()\n",
    "    doc = [ docs + ' 12345678 ' for docs in list(text_data['text'])]\n",
    "    # ------------------------- Stanford Named Entity Recognition\n",
    "    tokens = nltk.word_tokenize(str(doc))\n",
    "    entities = st.tag(tokens) # actual tagging takes place using Stanford NER algorithm\n",
    "\n",
    "\n",
    "    entities = [list(elem) for elem in entities] # Convert list of tuples to list of list\n",
    "    print ('tag complete')\n",
    "    for idx,element in enumerate(entities):\n",
    "        try:\n",
    "            if entities[idx][0] == '12345678':\n",
    "                entities[idx][1] = \"DOC_NUMBER\"  #  Modify data by adding the tag \"Doc_Number\"\n",
    "            #elif entities[idx][0].lower() == keyword:\n",
    "            #    entities[idx][1] = \"KEYWORD\"\n",
    "            # Combine First and Last name into a single word\n",
    "            elif entities[idx][1] == \"PERSON\" and entities[idx + 1][1] == \"PERSON\":\n",
    "                entities[idx + 1][0] = entities[idx][0] + '-' + entities[idx+1][0]\n",
    "                entities[idx][1] = 'Combined'\n",
    "            # Combine consecutive Organization names\n",
    "            elif entities[idx][1] == 'ORGANIZATION' and entities[idx + 1][1] == 'ORGANIZATION':\n",
    "                entities[idx + 1][0] = entities[idx][0] + '-' + entities[idx+1][0]\n",
    "                entities[idx][1] = 'Combined'\n",
    "        except IndexError:\n",
    "            break\n",
    "    print ('enumerate complete')\n",
    "    # Filter list of list for the words we are interested in\n",
    "    filter_list = ['DOC_NUMBER','PERSON','LOCATION','ORGANIZATION']\n",
    "    entityWordList = [element for element in entities if any(i in element for i in filter_list)]\n",
    "\n",
    "    entityString = ' '.join(str(word) for insideList in entityWordList for word in insideList) # convert list to string and concatenate it\n",
    "    entitySubString = entityString.split(\"DOC_NUMBER\") # split the string using the separator 'TWEET_NUMBER'\n",
    "    del entitySubString[-1] # delete the extra blank row created in the previous step\n",
    "\n",
    "    # Store the classified NERs in the main tweet data frame\n",
    "    for idx,docNER in enumerate(entitySubString):\n",
    "        docNER = docNER.strip().split() # split the string into word list\n",
    "        # Filter for words tagged as Organization and store it in data frame\n",
    "        text_out.loc[idx,'Organization'] =  ','.join([docNER[i-1]  for i,x in enumerate(docNER) if x == 'ORGANIZATION'])\n",
    "        # Filter for words tagged as LOCATION and store it in data frame\n",
    "        text_out.loc[idx,'Place'] = ','.join([docNER[i-1] for i,x in enumerate(docNER) if x == 'LOCATION'])\n",
    "        # Filter for words tagged as PERSON and store it in data frame\n",
    "        text_out.loc[idx,'Person'] = ','.join([docNER[i-1]  for i,x in enumerate(docNER) if x == 'PERSON'])\n",
    "\n",
    "    print ('process complete')\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start get_NER\n",
      "tag complete\n",
      "enumerate complete\n",
      "process complete\n"
     ]
    }
   ],
   "source": [
    "text_ner_out = get_NER(text_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Organization</th>\n",
       "      <th>Place</th>\n",
       "      <th>Person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>lenders refused to offer the couple of mortgag...</td>\n",
       "      <td></td>\n",
       "      <td>dubai</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>shadow housing minister to outlaw ‘profiteerin...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>john-healey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>meanwhile, back in the house of commons, back ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>bla-bla-bla-bla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674</th>\n",
       "      <td>the situation in the u.k. and pakistan is not ...</td>\n",
       "      <td></td>\n",
       "      <td>pakistan</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397</th>\n",
       "      <td>nice to see sky's coverage of liverpool v arse...</td>\n",
       "      <td>liverpool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text Organization  \\\n",
       "73    lenders refused to offer the couple of mortgag...                \n",
       "1472  shadow housing minister to outlaw ‘profiteerin...                \n",
       "1597  meanwhile, back in the house of commons, back ...                \n",
       "1674  the situation in the u.k. and pakistan is not ...                \n",
       "2397  nice to see sky's coverage of liverpool v arse...    liverpool   \n",
       "\n",
       "         Place           Person  \n",
       "73       dubai                   \n",
       "1472                john-healey  \n",
       "1597            bla-bla-bla-bla  \n",
       "1674  pakistan                   \n",
       "2397                             "
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the outputs of the ner tagger\n",
    "text_ner_out.loc[(text_ner_out['Place'] != '') | (text_ner_out['Organization'] != '')|(text_ner_out['Person'] != '')][['text','Organization','Place','Person']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output  the file for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_ner_out.to_csv('../processed_data/tweets_topics_sentiment_ner.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
